---
title: "DATA607 Natural Language Processing"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    theme: united
    highlight: tango
---


# Introduction
In this assignment I will be replicating the Sentiment Analysis example from Chapter 2. I will also be extending the example to include another corpus using a new lexicon. For the purposes of this assignment I have selected a the 2009 State of the Union Address and the Syuzhet lexicon.

- Corpus - I selected my corpus from a Kaggle data set that captures State of the Union Addresses from 1790 - 2018.  https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r/data
- Lexicon - I selected the from the syuzhet sentiment extractor from the Syuzhet package https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html




```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
rm(list=ls())

# load packages
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(reshape2)

library(readr)
library(SnowballC)
library(lexicon)
library(syuzhet)

```




# Chapter 2 Sentient Analysis Example
This section recreates the sentiment analysis example in chapter 2 of the Text Minging with R textbook. It includes examples of 3 different lexicons bing, afinn and nrc.




```{r}
#***************************************************************************************
#    Title: Text Mining with R : 02-sentiment-analysis
#    Author: Julia Silge and David Robinson
#    Date: Apr 6, 2021
#    Code version: #93
#    Availability: https://github.com/dgrtwo/tidy-text-mining/blob/master/02-sentiment-analysis.Rmd
#***************************************************************************************/

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

```





```{r}

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
        linenumber = row_number(),
        chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
        ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

nrcjoy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE)


```




```{r}


janeaustensentiment <- tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    count(book, index = linenumber %/% 80, sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(sentiment = positive - negative)


ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")



```


```{r}

pride_prejudice <- tidy_books %>%
  filter(book == "Pride & Prejudice")

pride_prejudice

afinn <- pride_prejudice %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber %/% 80) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
    pride_prejudice %>%
        inner_join(get_sentiments("bing")) %>%
        mutate(method = "Bing et al."),
    pride_prejudice %>%
        inner_join(get_sentiments("nrc") %>%
                 filter(sentiment %in% c("positive",
                                         "negative"))) %>%
    mutate(method = "NRC")) %>%
    count(method, index = linenumber %/% 80, sentiment) %>%
    spread(sentiment, n, fill = 0) %>%
    mutate(sentiment = positive - negative)


bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")


get_sentiments("nrc") %>%
    filter(sentiment %in% c("positive","negative")) %>%
    count(sentiment)

get_sentiments("bing") %>%
  count(sentiment)



```





```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

bing_word_counts %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to sentiment",x = NULL) +
    coord_flip()




```




```{r}
custom_stop_words <- bind_rows(data_frame(
                            word = c("miss"),
                            lexicon = c("custom")),
                            stop_words)

custom_stop_words




```




```{r}


tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))


```



```{r}

#***************************************************************************************
#    Title: Text Mining with R : 02-sentiment-analysis
#    End of Code block
#***************************************************************************************/

```



# Import data
I imported the data directly from the Kaggle's Google repository. I divided the speech up into sentences for the initial analysis.

```{r}


file <- "https://storage.googleapis.com/kagglesdsdata/datasets/1660/2921/Obama_2009.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211031%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211031T015335Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=95ba89f675fc7648f5011505375b91caae8f48d9c9b3330c50d284869d16106771e5caff376a85ac71e6944a44e82e46578f5e89aed4119a729bd890c7ac945572805d0bf2a2568aa1c35604b8f8d633071b2cf95134a9fdb8061368b26259d507b1571dbe63a66a1da4b012f710c8d3f3d8dd443ba50f20d501060bf8b3f94ab4b35f5311536ca4a7278c2ea07f54a2431c6f59d1cbff2667026088c6290f0dbfc0076edea6fdefd2439e114958a423dd5351069edf1007c515b296d1e1e7f262818e35d4743af6e935bb12947a0fb342221dfb5e786c7a690c3e4c37dc36ca32d2da6554a64da55ba7ba76028c33fb2e05e41a0830d77043450033a3e37e0b"

state_df <- read_delim(file, delim = "\\." , col_names = c("value"))


```



# Tiddy and Tranform Data
The text data is clean but I added a line number for future reference.
- add line number
- unnest tokens - each row will represent a single word
- filter out stop words

```{r}


# add columns
state_df <- state_df %>% 
    mutate(
        line_num = row_number()
    ) %>%
  unnest_tokens(word, value)

        

# filter stop words
pres_stop_words <- bind_rows(
    tibble(word = c("miss"),  
    lexicon = c("pres")), 
    stop_words
)

state_df <- state_df %>%
  anti_join(pres_stop_words)
    
```




# Modeling
The Syuzhut lexicon returns a vector of sentiment scores that includes 0.0 values. I filtered these scores out because they did not add to the analysis. I also tried to map the same sentiment scores to sentiment scores for afinn lexicon 
- get sentiment from syuzhet lexicon
- get sentiment from afinn lexicon
- get stem word sentiment from syuzhet lexicon
- calculate word data frame
- calculate z scores for comparison purposes


```{r}



# add the syuzhet sentiment
state_df$value_syuzhet <-  get_sentiment(state_df$word, method="syuzhet")
state_df <- state_df %>% filter(value_syuzhet != 0)


# add sentiments and stem words (afinn)
state_df <- state_df %>%
    left_join(get_sentiments("afinn")) %>%
    mutate(
      stem = wordStem(word)
    )


# add sentiments for stem words (afinn)
state_df$value_stem <-  get_sentiment(state_df$stem, method="syuzhet")


# create a word based dataframe
word_df <- state_df %>%
    select(-c(line_num,stem, value_stem)) %>%
    group_by(word) %>%
    mutate (
        freq = n(),
        val_freq = value * freq,
    ) %>%
    distinct()


# calculate the z values for comparision
m_value <- mean(word_df$value, na.rm = TRUE)
sd_value  <- sd(word_df$value, na.rm = TRUE)
m_value_syuzhet <- mean(word_df$value_syuzhet)
sd_value_syuzhet <- sd(word_df$value_syuzhet)

word_df <- word_df %>%
    mutate(
        z_value = (value - m_value) / sd_value,
        z_value_syuzhet = (value_syuzhet - m_value_syuzhet) / sd_value_syuzhet,
        z_diff = z_value - z_value_syuzhet
    )


```




# Visualize (full speach)
The sentiment for the full state of the union narrative seems to capture the somber tone that was in the country at the time. With the Financial system and the housing market still in crisis the speech starts on a somber note but increases in positive before settling at a neutral tone. This is also reflected in the word cloud with a balance of negative words debt, crisis, recession with positive words such as opportunity, confidence and care

```{r}

simple_plot(state_df$value_syuzhet,
            title = "Sentiment Analysis of the State of the Union Address 2009",
            legend_pos = "topleft"
            )



```



```{r}

state_df %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, random.color=TRUE))

  
```






# visualization of word choice
The whole speech had an emphasis on Care. It is the biggest work in the word cloud and if you look a frequency and sentiment it is by far the focus of the speech follow by crisis and recession. 



```{r}

word_df %>%   
ggplot(aes(word, value_syuzhet*freq)) +
    geom_col(show.legend = FALSE) +
    scale_fill_viridis_c() + 
    theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())

# words that are outliers in terms of sentiment and frequencey
word_df %>%
filter(abs(value_syuzhet*freq) > 2) %>%
ggplot(aes(word, value_syuzhet*freq)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    scale_fill_viridis_c() + 
    theme(axis.ticks.x = element_blank(),
        axis.text.x = element_text()) 



```



# comparison of lexicons (afinn vs syuzhet)
Calculating the z scores for each word sentiment values allow us to compare the relative sentiment scores by each lexicon using the same scale. What we can see is that the difference lexicons had discrepancies with the following words. With Afinn lexicon being more positive for words to the right of 0 and Syushet being more positive for word on the left of zero. Given this variability it is important to test more than on lexicon.

```{r}
# words that are outliers in terms of sentiment and frequencey
word_df %>%
filter(abs(z_diff) > .8) %>%
ggplot(aes(word, z_diff)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    scale_fill_viridis_c() + 
    theme(axis.ticks.x = element_blank(),
        axis.text.x = element_text()) 
```





# Conclusion
The exercise highlighted a few areas of sentiment analysis that I wanted to focus on. Since I was working with a speech I started with the overall flow. Looking at how the sentiment changed overtime as the speech progressed. The sentiment scores seem consistent with the narrative arch of the speech. I also looked at the content overall focusing on word choice and frequency of word usage. It was interesting to see the focus on "Care" as a consistent theme across the speech. It seems like stem analysis added very little to the overall analysis. There was a low percentage of matches for the stem words. I would like to explore ways to increase the relevance of this analysis going forward. 







# References
- Silge, J and Robinson, D (2017) Text Mining with R: 02-sentiment-analysis (3) [source Code]. https://github.com/dgrtwo/tidy-text-mining/blob/master/02-sentiment-analysis.Rmd
- Tatman, R (2017) Tutorial: Sentiment Analysis in R: State of the Union Corpus (1790 - 2018) (8) [Dataset]. https://www.kaggle.com/rtatman/tutorial-sentiment-analysis-in-r/notebook







